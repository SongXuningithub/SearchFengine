#!/usr/bin/env python3
"""
é‡‘èæœç´¢å¼•æ“æ¼”ç¤ºè„šæœ¬
å±•ç¤ºæ–°çš„batchçˆ¬è™«å’Œæ‰©å……çš„URLåˆ—è¡¨
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config.settings import FINANCIAL_SEED_URLS
from crawler.crawler import BatchCrawler

async def demo():
    print("ğŸš€ é‡‘èæœç´¢å¼•æ“æ¼”ç¤º")
    print("=" * 50)
    
    print(f"ğŸ“Š ç§å­URLæ•°é‡: {len(FINANCIAL_SEED_URLS)}")
    print(f"ğŸŒ è¦†ç›–ç½‘ç«™ç±»å‹:")
    
    # ç»Ÿè®¡ç½‘ç«™ç±»å‹
    categories = {
        'å›½é™…ä¸»æµè´¢ç»åª’ä½“': ['cnbc.com', 'bloomberg.com', 'reuters.com', 'ft.com', 'wsj.com'],
        'ä¸“ä¸šé‡‘èç½‘ç«™': ['investing.com', 'tradingview.com', 'seekingalpha.com'],
        'åŠ å¯†è´§å¸': ['cointelegraph.com', 'coindesk.com', 'coingecko.com'],
        'äºšæ´²è´¢ç»åª’ä½“': ['scmp.com', 'straitstimes.com', 'channelnewsasia.com'],
        'æ¬§æ´²è´¢ç»åª’ä½“': ['handelsblatt.com', 'lesechos.fr', 'expansion.com'],
        'é‡‘èç§‘æŠ€': ['fintechfutures.com', 'finextra.com', 'bankingtech.com'],
        'å®è§‚ç»æµ': ['imf.org', 'worldbank.org', 'oecd.org', 'bis.org'],
        'å¯æŒç»­é‡‘è': ['environmental-finance.com', 'responsible-investor.com', 'greenbiz.com']
    }
    
    for category, domains in categories.items():
        count = sum(1 for url in FINANCIAL_SEED_URLS 
                   if any(domain in url for domain in domains))
        if count > 0:
            print(f"   â€¢ {category}: {count} ä¸ªç½‘ç«™")
    
    print("\nğŸ”§ æŠ€æœ¯ç‰¹æ€§:")
    print("   â€¢ Batchå¤„ç†æ¨¡å¼ (é»˜è®¤8ä¸ªURLå¹¶å‘)")
    print("   â€¢ SQLiteæ•°æ®åº“å­˜å‚¨")
    print("   â€¢ æ™ºèƒ½é‡‘èå†…å®¹è¯†åˆ«")
    print("   â€¢ æ”¯æŒä¸­è‹±æ–‡æ··åˆå†…å®¹")
    
    print("\nğŸ“ˆ æ”¯æŒçš„é‡‘èé¢†åŸŸ:")
    print("   â€¢ è‚¡ç¥¨ã€å€ºåˆ¸ã€åŸºé‡‘ã€æœŸè´§ã€æœŸæƒ")
    print("   â€¢ å¤–æ±‡ã€é»„é‡‘ã€åŸæ²¹ã€å¤§å®—å•†å“")
    print("   â€¢ åŠ å¯†è´§å¸ã€åŒºå—é“¾ã€DeFi")
    print("   â€¢ é‡‘èç§‘æŠ€ã€æ•°å­—é“¶è¡Œ")
    print("   â€¢ ESGã€å¯æŒç»­é‡‘è")
    print("   â€¢ æ–°å…´å¸‚åœºã€å®è§‚ç»æµ")
    
    print("\nğŸ¯ ä½¿ç”¨ç¤ºä¾‹:")
    print("   # å¯åŠ¨çˆ¬è™« (batch_size=8)")
    print("   python -m crawler.main --max-pages 100 --batch-size 8")
    print("   ")
    print("   # å¯åŠ¨ç´¢å¼•å™¨")
    print("   python -m indexer.main")
    print("   ")
    print("   # å¯åŠ¨æœç´¢å¼•æ“")
    print("   python -m engine.main")
    print("   ")
    print("   # å¯åŠ¨å‰ç«¯")
    print("   python -m frontend.app")
    
    print("\nâœ¨ ä¸€é”®å¯åŠ¨:")
    print("   ./start.sh")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(demo())
